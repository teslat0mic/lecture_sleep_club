


















1
00:00:00,000 --> 00:00:02,801
Okay, I'm going back to where we were now.

2
00:00:03,120 --> 00:00:08,801
And I felt that I didn't do the criticism of naturalism very well.

3
00:00:08,801 --> 00:00:18,400
I'm certainly not going to go back over it and read the quotes I was reading, which I felt as I read them didn't actually nail the point I wanted to make anyway.

4
00:00:18,641 --> 00:00:23,280
I think I can make the important point simply.

5
00:00:23,601 --> 00:00:38,081
And about naturalism, remember, is the general criticism that you can't understand Dasein in terms of what you can get about, what you can discover about nature by way of natural science.

6
00:00:38,081 --> 00:00:40,321
He thinks that's impossible.

7
00:00:40,321 --> 00:00:47,920
Go back to page 94, where he says that in very cryptic terms, and then go on from there.

8
00:00:47,921 --> 00:01:08,720
So on 94, he says, three lines down, near the bottom of 65 in the German, this manner of knowing them, that knowing them as present at hand, as nature, has the character of depriving the world of its worldhood in a definite way.

9
00:01:08,720 --> 00:01:17,361
That means desituating, deworlding, get it out of any relation to our us and our practices and purposes.

10
00:01:17,361 --> 00:01:22,081
Then you get the kind of nature that science deals with.

11
00:01:22,720 --> 00:01:34,120
And the claim is, next sentence, nature as the categorial aggregate of those structures of being which a definite entity encountered within the world may possess.

12
00:01:34,441 --> 00:01:59,960
That means nature as the sort of way of being that all the stuff from electrons to galaxies have, namely there being present-at-hand substances with present-at-hand properties, those, whatever they are, whatever properties you have, and whatever substance you have, you can never make worldhood intelligible.

13
00:01:59,960 --> 00:02:15,960
The thesis is that to get to that kind of ready presence at hand, you have to leave out all the holistic, meaningful, situatedness that goes with worldhood.

14
00:02:15,960 --> 00:02:27,160
And once you leave it out, I'm not sure why he thinks, and that's when I was worried last time, I don't really know why he says it's impossible to get it back.

15
00:02:27,160 --> 00:02:31,160
I think it's extremely implausible to think you can get it back.

16
00:02:31,160 --> 00:02:36,040
But it would be nice to know what the impossibility argument is.

17
00:02:36,360 --> 00:02:38,441
I can make up half of it.

18
00:02:38,440 --> 00:02:42,761
That is, suppose, this came up in a discussion after class last time.

19
00:02:42,761 --> 00:02:57,240
Suppose science just declares that they're not interested in the phenomena, that is our experience of meaning, our experience of worldhood, our consciousness of all of the phenomena.

20
00:02:57,241 --> 00:03:05,240
They will just explain the meaningless movements of the meaningless particles that make up reality.

21
00:03:05,241 --> 00:03:07,800
And they'll explain us as objects.

22
00:03:07,800 --> 00:03:13,480
And they'll just finesse what in the earlier days were called the secondary qualities.

23
00:03:13,481 --> 00:03:16,881
They'll just finesse all the stuff that we've been talking about.

24
00:03:14,361 --> 00:03:20,561
Now, Heidegger, I think, has an argument that that's impossible.

25
00:03:21,201 --> 00:03:27,920
Science can't just ignore the phenomena and count them as mere experiences.

26
00:03:27,920 --> 00:03:33,761
And he's got that.

27
00:03:34,321 --> 00:03:39,121
And the idea is that you can't get to the present at hand.

28
00:03:39,121 --> 00:03:41,281
You have no access to the present at hand.

29
00:03:41,280 --> 00:03:53,521
You can't make sense of the present at hand except on the background of worldhood and our practices for having intentionality and making sense of anything.

30
00:03:53,521 --> 00:03:58,240
So you can't just say that's just appearances, we don't have to account for it.

31
00:03:58,241 --> 00:04:07,441
Science has to account for it if they're going to get an understanding of where their understanding comes from.

32
00:04:08,001 --> 00:04:16,561
And he says at one point, but I couldn't find it, but you keep your eye open for it, we get to the present at hand through the ready-to-hand.

33
00:04:16,560 --> 00:04:22,240
And I think that means for him, we can only make the present at hand intelligible.

34
00:04:22,241 --> 00:04:29,840
And that was what you get when you leave out the situatedness, the world, and all that that goes with the ready-to-hand.

35
00:04:30,241 --> 00:04:38,641
But you need the whole ready-to-hand worldhood to account for how we can study anything, including science.

36
00:04:38,641 --> 00:04:52,240
There's a footnote in the margin of 94 that might have something to do with this, where he says that nature can never make worldhood intelligible.

37
00:04:52,241 --> 00:04:57,441
He writes in the margin, rather, the reverse exclamation mark.

38
00:04:57,441 --> 00:04:59,040
That's what I've just been saying.

39
00:04:59,040 --> 00:05:05,081
That is, the only way that you can make nature intelligible is by way of worldhood.

40
00:05:05,321 --> 00:05:08,841
And that's in a sentence.

41
00:05:08,840 --> 00:05:12,761
He's trying to stress that in the marginal remarks.

42
00:05:13,160 --> 00:05:17,321
I don't know whether, did I tell you this already, that there are marginal remarks.

43
00:05:17,321 --> 00:05:21,481
They're not all that fascinating, and when they are, I'll bring them in.

44
00:05:21,480 --> 00:05:41,081
But he spent every once in a while, and he doesn't have any dates, he wrote in his copy of Being in Time that he had in his ski hut in the mountains where he went to think, and he would write marginal comments, and now they are translated in the Joan Stanbaugh translation as footnotes.

45
00:05:41,081 --> 00:05:46,360
I don't recommend you buy the Stanbaugh translation because I think the one that you're reading is better.

46
00:05:46,361 --> 00:05:48,761
But I will put it on, did I say this already?

47
00:05:48,761 --> 00:05:53,880
I should put it on reserve so that if you want to go look at the footnotes, you can.

48
00:05:53,881 --> 00:05:54,840
And I'll do that.

49
00:05:55,081 --> 00:05:58,761
I'll put my copy of Stanbaugh on reserve.

50
00:05:58,761 --> 00:06:06,201
And now on 122, why do I want to say we should look at that as a punchline?

51
00:06:08,121 --> 00:06:10,280
Yeah, that's the punchline now.

52
00:06:10,680 --> 00:06:28,681
In case you aren't convinced by what I've been saying so far about the way science depends on the phenomena that Heidegger is describing in order for us to be able to do it at all, remember over and over, that doesn't mean the entities that science deals with depends on our practices and so forth.

53
00:06:28,680 --> 00:06:34,841
There's a very tricky point, which I keep coming back to, and we will get back to when we get to the reality chapter.

54
00:06:34,840 --> 00:06:49,841
The intelligibility, the understanding of being that enables us to encounter and study electrons depends on us, but electrons go right on even if we would never existed and have their weight and charge and everything, Heidegger thinks, I'm sure.

55
00:06:50,160 --> 00:06:55,441
But most people don't believe this, so whenever I come across a place, I read it.

56
00:06:55,440 --> 00:07:02,321
Here's a place where, so let me read something about 10 lines down on 122.

57
00:07:02,641 --> 00:07:06,801
It's about 88 in the German and bottom of it.

58
00:07:06,800 --> 00:07:20,400
He says, and only if entities within the world can be encountered at all is it possible in the field of such entities to make accessible what is just present at hand and no more.

59
00:07:20,400 --> 00:07:22,400
See, it's about accessibility.

60
00:07:22,400 --> 00:07:25,121
It's not about there were any electrons.

61
00:07:25,121 --> 00:07:32,001
I mean, you make accessible electrons as just present at hand and no more, but you have to get there with it.

62
00:07:32,241 --> 00:07:38,400
That only becomes accessible in the field of meaning and so forth.

63
00:07:38,400 --> 00:07:49,920
By reason of being, now comes another important thing, by reason of being just present at hand and no more, these latter entities can have their properties defined mathematically in functional concepts.

64
00:07:49,920 --> 00:07:57,601
And ontologically, such concepts are possible only in relation to entities whose being has the character of pure substantiality.

65
00:07:57,601 --> 00:08:01,841
All that's about beings like electrons and galaxies.

66
00:08:01,840 --> 00:08:07,761
You get access to them, and then you can get, you can, then you find these pure substances.

67
00:08:07,761 --> 00:08:26,721
And, but, but I'm just going back and repeating, you cannot explain in terms of those pure substances the experience of meaning and worldhood and openness to objects and so forth that made you able to get access to these pure present at hand things in the first place.

68
00:08:26,721 --> 00:08:29,041
Okay, it's the best I can do about that.

69
00:08:29,040 --> 00:08:31,481
I'm going on to the next round.

70
00:08:29,280 --> 00:08:36,761
For the next move is: okay, so naturalism fails.

71
00:08:37,080 --> 00:08:41,801
Well, Descartes was never a naturalist anyway.

72
00:08:42,121 --> 00:08:51,321
He didn't think you could explain human consciousness and meaning and so forth in terms of the present at hand.

73
00:08:51,320 --> 00:08:57,641
He didn't think you could explain it in terms, oh, sorry, explain me.

74
00:08:57,961 --> 00:08:59,881
Well, no, he didn't.

75
00:08:59,881 --> 00:09:02,920
He didn't think you could explain it in terms of the present at hand.

76
00:09:02,920 --> 00:09:12,441
He needed another substance to explain how we can have a world of meaningful objects.

77
00:09:12,440 --> 00:09:17,481
And that was the res cogitans, the thinking substance.

78
00:09:17,481 --> 00:09:19,560
That's Cartesian dualism.

79
00:09:19,560 --> 00:09:21,081
It's still around.

80
00:09:21,080 --> 00:09:34,920
I mean, in a certain, much more refined and modern and up-to-date way, it's still in, it was certainly in Husserl, I'll show you, who's Heidegger's teacher, and it's still in Searle.

81
00:09:34,920 --> 00:09:42,601
There has to be both minds and stuff, and minds give meaning to the brute stuff.

82
00:09:43,241 --> 00:09:45,400
That goes through all of this.

83
00:09:45,400 --> 00:09:55,001
Now, I'm going to just, on the way there, touch base with something I didn't read last time, which I should have read, then which I was just presupposing.

84
00:09:55,001 --> 00:10:08,920
On page 125, Heidegger starts his account of Descartes and Descartes' attempt to understand the world by defining substance.

85
00:10:08,920 --> 00:10:12,761
I've been saying this over and over, but it's time to see it in print.

86
00:10:12,761 --> 00:10:35,841
So it's at the beginning of section 20 on page 125, in the third sentence, German, top of 92, he's translating Descartes' Latin, and he says, by substance we can understand nothing else than an entity which is in such a way that it needs no other entity in order to be.

87
00:10:35,841 --> 00:10:41,601
And Heidegger's underlining is and be because he's saying this is an ontological claim.

88
00:10:41,601 --> 00:10:43,921
This is Descartes' ontology.

89
00:10:43,920 --> 00:10:54,081
This has been the ontology of everybody since Aristotle, that the ultimate sort of building blocks of reality are substances.

90
00:10:54,080 --> 00:11:00,481
That's what there finally really is, is whatever there is that is self-sufficient.

91
00:11:00,481 --> 00:11:04,320
And that what there really is, now that's the general ontology.

92
00:11:04,320 --> 00:11:08,400
Then you have to look and see, well, what kinds of beings are self-sufficient?

93
00:11:08,400 --> 00:11:22,480
And it turns out, namely, res extensa, the bits of matter that are moving around, now strings or quarks, those are bits, whatever they are, self-sufficient.

94
00:11:22,481 --> 00:11:26,721
Descartes thought they were bits of just extended matter.

95
00:11:26,721 --> 00:11:29,361
They were sort of atoms for Descartes.

96
00:11:29,361 --> 00:11:41,361
The atoms moving around have substance as their way of being, and now we have to add, and minds are also self-sufficient and have substance as their way of being.

97
00:11:42,080 --> 00:12:02,921
From Descartes right through to Searle, minds are so self-sufficient that you could have stuff going on in minds with where there was even no world at all, sort of the matrix picture, where all of reality is really a kind of big, complicated, interconnected illusion.

98
00:12:03,160 --> 00:12:10,361
And what there is finally, and in there, it's more, it's sort of brains, but it's also minds.

99
00:12:10,361 --> 00:12:12,681
Never mind those details.

100
00:12:14,200 --> 00:12:21,641
What we want to brains don't come into it when you do it from Descartes' angle or Husserl's.

101
00:12:21,641 --> 00:12:40,921
There's something called minds, and they have their own, it has its own status, namely consciousness, and it has its own capacity, namely intentionality, or transcendence in the old sense, not Heidegger's sense, namely pointing outside of itself to objects.

102
00:12:40,920 --> 00:12:52,601
But it can point outside of itself to objects, minds can, even if there aren't any objects, then everything it's pointing is wrong, and it's all in a kind of illusion, like the people in the matrix.

103
00:12:52,601 --> 00:12:55,160
But the mind is self-sufficient.

104
00:12:55,160 --> 00:13:01,001
It's going to go, it can go around doing its pointing to things, whether there are any things or not.

105
00:13:01,001 --> 00:13:02,920
I mean, think of Santa Claus.

106
00:13:02,920 --> 00:13:22,201
I mean, whether there's any Santa Claus or not, you can have a representation of it in your mind, and you can have intentionality about it in the sense that you believe that there's a Santa Claus, and you can even, if you're lucky, see Santa Claus coming down the chimney, but there isn't any Santa Claus, doesn't matter.

107
00:13:22,521 --> 00:13:29,961
Okay, now, so now we've got brute facts in Searle language.

108
00:13:29,961 --> 00:13:32,601
That's the extended things.

109
00:13:32,920 --> 00:13:37,641
And we've got what in Searle language, intrinsic intentionality.

110
00:13:37,641 --> 00:13:41,561
That's the mind's capacity to give meaning to things.

111
00:13:41,560 --> 00:13:43,961
And how does it work in Descartes already?

112
00:13:43,961 --> 00:13:46,801
And in Searle, it hasn't changed a bit.

113
00:13:44,441 --> 00:14:07,521
Well, that the minds give meanings, value predicates, in Searle language, function predicates, to bits of matter, and thereby treat matter as electrons and chairs and chalk and so forth.

114
00:14:07,521 --> 00:14:17,361
And that's the crucial move for getting worlds out of substances, of these two substances.

115
00:14:17,361 --> 00:14:27,681
And now, it's so important to just see that, and particularly since, amazingly enough, in Searle's book on social reality, it still sounds exactly like this.

116
00:14:28,001 --> 00:14:36,241
I'm going to read you the bottom of 131 in German, the bottom of 98.

117
00:14:36,241 --> 00:14:41,121
I'm going to read a long passage, but it's just straightforward what I've been saying.

118
00:14:41,121 --> 00:14:54,640
Descartes has still laid the basis for characterizing ontologically that entity within the world upon which, in its very being, every other entity is founded, namely material nature.

119
00:14:54,640 --> 00:15:01,440
This would be the fundamental stratum upon which all the other strata of actuality within the world are built up.

120
00:15:01,441 --> 00:15:16,961
The extended thing, as such, would serve in the first instance as the ground for those definite characters which show themselves, to be sure, as qualities, which at bottom are quantitative modifications of the modes of the extensio.

121
00:15:16,961 --> 00:15:20,320
That's the extended, that's the brute facts.

122
00:15:20,320 --> 00:15:24,640
These qualities, now, these brute facts seem to have qualities.

123
00:15:24,640 --> 00:15:26,320
That's the interesting problem.

124
00:15:26,320 --> 00:15:34,521
These quantity, and that means that there are somehow qualities, not just quantity modifications.

125
00:15:34,840 --> 00:15:37,160
And now we have to figure out that can be.

126
00:15:37,161 --> 00:15:53,880
These qualities, which are themselves reducible somehow to quantities, would provide the footing for such specific qualities as beautiful, ugly, in keeping, not in keeping, useful, and useless.

127
00:15:53,880 --> 00:16:06,280
The useful is the main thing, because that means hammers, chairs that are working or that are broken, all of that is something the mind can attribute to things.

128
00:16:06,280 --> 00:16:13,481
Now, there's something that bothered me as I went by that has surprised me.

129
00:16:15,241 --> 00:16:26,120
The stuff that shows itself as qualities, which is at bottom quantitative modifications of the modes of extensio itself, why is that?

130
00:16:26,120 --> 00:16:35,080
I mean, in Searle and in Husserl, and I would have thought in Descartes, the qualities are brought in by the mind substance.

131
00:16:35,080 --> 00:16:39,640
Why would they be modifications of the extensio itself?

132
00:16:39,640 --> 00:16:41,960
Maybe that's hmm.

133
00:16:46,521 --> 00:16:49,080
Hold his face words and provided by the mind.

134
00:16:49,161 --> 00:16:49,960
He seems indignant.

135
00:16:50,040 --> 00:16:55,481
Color is part of the objects themselves, and so we try to explain color in some things.

136
00:16:55,880 --> 00:16:57,560
Ah, I didn't know that.

137
00:16:57,560 --> 00:17:00,440
Okay, well, that must be what this sentence is about.

138
00:17:03,961 --> 00:17:05,161
Ah, okay.

139
00:17:05,481 --> 00:17:13,080
So it's funny, though, that it can sort of misleading because color isn't on his list of beautiful, ugly, useful, and so forth.

140
00:17:13,080 --> 00:17:15,680
And that really confuses me.

141
00:17:14,120 --> 00:17:21,360
But at least this much is true, and Adam and I and Heidegger are all agreeing on this.

142
00:17:21,681 --> 00:17:35,680
Some of the value qualities of things can't be gotten by putting together the extended stuff, no matter how cleverly that you can't get out of that the beautiful, the useful, and so forth.

143
00:17:36,080 --> 00:17:37,120
That's the dualism.

144
00:17:37,761 --> 00:17:42,880
You have to bring in something different from this other kind of substance.

145
00:17:42,880 --> 00:17:44,481
I think that's right, isn't it?

146
00:17:44,481 --> 00:17:47,360
I mean, otherwise, you'd have a monism.

147
00:17:47,600 --> 00:17:51,280
Okay, so now, and now let me go at the top of 32.

148
00:17:51,280 --> 00:17:59,681
If one is oriented primarily by thinghood, these latter qualities must be taken as non-quantifiable value predicates.

149
00:17:59,681 --> 00:18:03,761
Aha, he answers my complaint in the next sense.

150
00:18:03,761 --> 00:18:04,641
You were going to say that?

151
00:18:04,640 --> 00:18:05,840
Somebody's nodding.

152
00:18:06,401 --> 00:18:17,280
So these value predicates aren't ultimately quantifiable, by which what is, in the first instance, just a material thing, gets stamped as something good.

153
00:18:17,280 --> 00:18:23,360
Now, don't read as good, meaning good sort of in the sense of valuable or something.

154
00:18:23,681 --> 00:18:25,441
You read good for something.

155
00:18:25,441 --> 00:18:29,441
So good for sitting or good for putting your books on when you're lecturing.

156
00:18:29,441 --> 00:18:35,040
These are function predicates mostly, or you needn't think of anything except function predicates.

157
00:18:35,040 --> 00:18:41,601
And if then in Searle's, what is it, the, what's the name of the social reality book?

158
00:18:41,600 --> 00:18:47,761
The construction of social reality, he concentrates on the function predicates.

159
00:18:48,001 --> 00:18:48,640
I'm going on.

160
00:18:48,640 --> 00:18:54,401
With this stratification, we come to those entities we've characterized ontologically as equipment ready to hand.

161
00:18:54,401 --> 00:18:57,040
You saw me just talk about the lecture and the chairs.

162
00:18:57,040 --> 00:19:13,080
The Cartesian analysis of the world, where world means the totality of things, would thus enable, remember the quotes, when world is in quotes, it isn't Heidegger world, it's the wrong view of the world that people have had up to Heidegger.

163
00:19:14,201 --> 00:19:23,721
The Cartesian analysis of the quotes world would thus enable us for the first time to build up securely the structure of what is proximally ready to hand.

164
00:19:23,721 --> 00:19:32,360
All it takes is to round out the thing of nature until it becomes a full-fledged thing of use, and this is easily done.

165
00:19:32,360 --> 00:19:37,721
Now, does anybody, I hope you're with it enough, to be stunned by that sentence.

166
00:19:38,040 --> 00:19:39,721
What's going on there?

167
00:19:42,040 --> 00:19:44,920
Why is he seemingly contradicting himself?

168
00:19:47,241 --> 00:19:48,280
It's not a trick question.

169
00:19:48,280 --> 00:19:50,521
It's just saying you've got to pay attention.

170
00:19:50,521 --> 00:19:53,400
Does Heidegger think it can be easily done?

171
00:19:53,401 --> 00:19:54,201
Not at all.

172
00:19:54,201 --> 00:19:55,721
He's being ironic.

173
00:19:55,721 --> 00:20:01,081
That's the view, that's the view that the scientists or John Searle has and Husserl.

174
00:20:01,080 --> 00:20:03,400
But it's not his view.

175
00:20:03,401 --> 00:20:10,920
And why it's sort of surprising is Heidegger's capacity for irony is pretty minimal.

176
00:20:11,800 --> 00:20:18,440
I don't remember any place else in Being in Time where he ever says anything that he doesn't think is just plain true.

177
00:20:18,441 --> 00:20:26,680
But here he lets himself really go crazy and assert the opposite of what he really believes.

178
00:20:27,320 --> 00:20:34,800
And there's a place where he says, and that is sort of funny, that in the margin, I wrote it down.

179
00:20:35,040 --> 00:20:35,960
Just a second.

180
00:20:35,961 --> 00:20:38,280
Well, let me go to it in the natural order.

181
00:20:38,280 --> 00:20:44,961
Now, how many, not very many, had Husserl last summer, or may have Husserl probably next summer.

182
00:20:45,201 --> 00:20:55,521
We're trying to at least have Husserl in the background for people who want to know Heidegger's teacher and what Heidegger is totally against.

183
00:20:56,481 --> 00:21:06,880
Anyway, Husserl does say, and in his book, Cartesian Meditations, fully so named, and this is just Husserl version of the same thing we just read.

184
00:21:06,880 --> 00:21:11,681
He says, an existent mere physical thing, such as a hammer.

185
00:21:11,681 --> 00:21:12,960
What has he been reading?

186
00:21:13,201 --> 00:21:16,560
In fact, he read Being in Time twice, but he didn't get it.

187
00:21:17,840 --> 00:21:21,601
He said, oh, this is just psychology and anthropology and sociology.

188
00:21:21,600 --> 00:21:23,761
It's not transcendental philosophy.

189
00:21:23,761 --> 00:21:25,201
Well, it's neither, of course.

190
00:21:25,201 --> 00:21:27,280
It's existential phenomenology.

191
00:21:27,280 --> 00:21:30,241
But Husserl doesn't have a category for things.

192
00:21:30,241 --> 00:21:37,681
So an existent mere physical thing, such as a hammer, is given in the synthesis of passive experience.

193
00:21:37,681 --> 00:21:40,320
Never mind, that's Husserl jargon.

194
00:21:40,320 --> 00:21:46,800
It's given to, quote, spiritual activities which begin with aspective grasping.

195
00:21:46,800 --> 00:21:48,320
Spiritual is a funny translation.

196
00:21:48,320 --> 00:21:52,721
I'm sure it's Geistig in German, and you could equally well say mental activities.

197
00:21:52,721 --> 00:21:54,001
And that would make more sense.

198
00:21:54,001 --> 00:21:58,481
There's nothing very spiritual about grasping, taking something to be a hammer.

199
00:21:59,201 --> 00:22:06,800
But it's perfectly fine that it's mental that enable you to take it as a hammer.

200
00:22:06,800 --> 00:22:19,680
And then in Stanbaugh, on page 91, there's a footnote to this, and a m wonderful footnote for me, I mean, because I've always said this.

201
00:22:19,681 --> 00:22:23,441
All this is intended as a critique of Husserl, he writes in the margin.

202
00:22:23,441 --> 00:22:26,001
Because Husserl is sort of out out of being in time.

203
00:22:26,001 --> 00:22:40,680
You never he's he's, as I said, I think once before, people wonder if they used to criticize me because I kept saying, well, this is all a critique of Husserl and Husserl's view of intentionality, and everybody says, well, no, it isn't even mentioned in there.

204
00:22:40,681 --> 00:22:45,481
And there's this dedication to Husserl for which heide says he owes everything.

205
00:22:45,481 --> 00:22:53,001
Well, the reason he's not mentioning Husserl, I think, is because he's writing for eternity, and rightly so.

206
00:22:53,001 --> 00:23:09,800
He's going to be the main philosopher remembered of the 20th century when Husserl was long forgotten, as far as I can see, just as a last-ditch Cartesian, and he isn't going to get entangled with even it's like not mentioning the presidential candidate for the other party.

207
00:23:09,800 --> 00:23:12,600
I mean, why should you give Husserl any publicity?

208
00:23:12,600 --> 00:23:20,680
But in the margin, he does say that, of course, it's all aimed at Husserl, and it's aimed at Husserl because the students have been listening to Husserl.

209
00:23:20,681 --> 00:23:34,201
So in his lectures in Basic Problems and in History of Concept of Time, it's all over, written all over that he's talking about Husserl, and that he's got a different, more primordial understanding of intentionality than Husserl.

210
00:23:34,201 --> 00:23:37,560
And here he's also aiming at Husserl.

211
00:23:37,880 --> 00:23:54,680
And so now Heidegger has to answer the people who say we can understand the world as brute nature and then just contribute as minds meaning and functionality, function predicates.

212
00:23:54,681 --> 00:24:02,440
And that means we can, those people are mainly Descartes, Husserl, Sartre, who's very close to Husserl, and Thurlow.

213
00:24:02,761 --> 00:24:08,521
I'm sure there are millions of others too, but those are the big ones that I know anyway.

214
00:24:08,521 --> 00:24:11,640
And his now, what is his answer?

215
00:24:11,640 --> 00:24:14,521
And I've said this once, but now we're going to have to spell it out.

216
00:24:15,040 --> 00:24:36,001
His answer is that when you understand how holistic a function like hammering is, that means how hammers only make sense in relation to nails, in relation to wood, in relation to frames, in relation to houses, and then the end of that for the sake of chain is, and people needing shelters.

217
00:24:36,001 --> 00:24:40,001
And remember, from my point of view, that's still the beaver chain, so to speak.

218
00:24:40,001 --> 00:24:43,361
But then you also have this further holism.

219
00:24:43,360 --> 00:24:45,521
This is what I was talking about last time.

220
00:24:45,521 --> 00:24:52,080
And it only makes sense for beings who take a stand on themselves as shelterers or something else.

221
00:24:52,401 --> 00:25:08,800
And that is so holistic that to think that you can just take a blob on the end of a piece of wood and add an isolated predicate that it's for hammering, that you would capture anything like what it is to be a hammer would just be absurd.

222
00:25:08,800 --> 00:25:21,201
I mean, once you see the phenomenon of what it is, what's involved in something being a hammer, you couldn't, the idea, remember value predicates are substance-like.

223
00:25:21,201 --> 00:25:37,920
They are self-sufficient function meanings which you pin onto a self-sufficient nature entity, and that's supposed to capture enough meaning of Heidi of hammering that you can, that we normally have.

224
00:25:37,921 --> 00:25:40,800
Well, Heidegger would just say no.

225
00:25:40,800 --> 00:25:46,880
They all presuppose the referential whole and significance.

226
00:25:46,881 --> 00:25:57,681
So on 132, at the bottom, in italics, in the German, I don't know where we got lost, their German has disappeared on that page.

227
00:25:57,840 --> 00:26:02,840
The very bottom of 99, 10 lines from the bottom of 132.

228
00:26:03,800 --> 00:26:31,640
And if we are to reconstruct this thing of use, which supposedly comes to us in the first instance with its skin off, that is, that's the brute fact, does not this always require that we previously take a positive look at the phenomenon whose whole, and that's better than totality, whose wholeness, whose wholism such a reconstruction is meant to restore.

229
00:26:31,640 --> 00:26:39,080
Meaning, once you get the phenomenology right, you see that the whole Cartesian project is wrong, impossible.

230
00:26:40,360 --> 00:27:14,680
Now, now comes, for the rest of the lecture, something that's only going to happen once in here, and that is a bit of the history of this, because it's so interesting that this is the point at which Heidegger became extremely relevant to people actually having a research program, which was in effect the attempt to do just this job of capturing everyday intelligibility using brute facts and function predicates.

231
00:27:14,681 --> 00:27:22,280
And that was, by this amazing accident, happening at MIT when I was teaching at MIT.

232
00:27:22,280 --> 00:27:53,840
So for a little while, Heidegger and reality, namely money from DARPA, the Defense Department, of about a million a year, comes to MIT and they're promising that they're going to be able to create artificial intelligence, that this, they're going to be able to use computers, and all they're going to have are brute facts and the function predicates and out of that they're going to make it this robot that's going to behave intelligently.

233
00:27:53,840 --> 00:28:04,400
And there I was teaching at MIT and I and reading Heidegger and believing his phenomenology and saying, that's impossible.

234
00:28:04,401 --> 00:28:06,320
You guys can't do that.

235
00:28:06,320 --> 00:28:27,120
And so there was this fascinating moment in which what would look like the most abstract talk about being in this by this German philosopher in this dense way had immediate relevance to a very popular and well-supported research program.

236
00:28:27,120 --> 00:28:30,320
And I'm going to talk about it now and give you quotes about it.

237
00:28:30,320 --> 00:28:33,201
And every once in a while I turn out to be in these quotes.

238
00:28:33,441 --> 00:28:39,840
I'm not trying to make propaganda for me, but Heidegger and me got very entangled at that point.

239
00:28:39,840 --> 00:28:43,121
And they didn't think of themselves as fighting Heidegger.

240
00:28:43,120 --> 00:28:45,200
They thought of themselves as fighting me.

241
00:28:45,201 --> 00:28:47,921
They tried to keep me from getting tenure at MIT.

242
00:28:48,080 --> 00:28:50,080
It was too late to get Heidegger.

243
00:28:51,840 --> 00:29:06,160
But they said that they had to get rid of me at MIT because I was giving a false appearance of plausibility and scientificness to this kind of crazy philosophy stuff.

244
00:29:06,161 --> 00:29:17,360
And that was very dangerous because people somebody might read this at DARPA, the that's the defense money people, and take away their their millions of dollars.

245
00:29:17,360 --> 00:29:29,721
At which point I was tempted to find somebody to dress up like somebody from DARPA and have lunch with them in the faculty club and scare the people from the the artificial intelligence project to death.

246
00:29:29,441 --> 00:29:30,921
Things were very tense.

247
00:29:31,080 --> 00:29:44,681
I mean, none of them would speak to me or could be seen speaking to me without getting into trouble, except one courageous one named Joe Weisenbaum, who was beginning to have doubts about AI, and he would meet me out at his home in Lexington.

248
00:29:44,681 --> 00:29:47,320
It was all completely bizarre.

249
00:29:48,840 --> 00:29:49,961
But I won.

250
00:29:49,961 --> 00:29:52,761
I got tenure, and they failed.

251
00:29:53,880 --> 00:29:55,880
They had, it was amazing.

252
00:29:55,880 --> 00:30:08,360
They had a research program which was exactly applied Cartesianism, and that research program had been refuted by Heidegger, and they just didn't know it.

253
00:30:08,360 --> 00:30:12,200
But they found out it's over that research program.

254
00:30:12,201 --> 00:30:17,960
Minsky said, Minsky was the head of MIT at that point, and my big opponent.

255
00:30:17,961 --> 00:30:29,481
And lately in Wired magazine, Minsky has said that artificial intelligence has been brain dead since the 70s when it encountered the problem of common sense knowledge.

256
00:30:30,761 --> 00:30:35,401
That has encountered the referential totality and collapsed.

257
00:30:35,401 --> 00:30:42,681
But it took them, say, 20 years or so to realize that they're never going to solve the problem of common sense knowledge.

258
00:30:42,681 --> 00:30:45,160
Now I want to tell you more about that.

259
00:30:45,161 --> 00:30:57,160
Because, as I say, this is the one time when you get to see this amazing fact that Heidegger is there in the background seeing all of these people doing all these crazy things, knowing that you can't do it that way.

260
00:30:57,160 --> 00:31:03,481
So the next person to consider is Douglas Lynnette, L-E-N-A-T.

261
00:31:04,600 --> 00:31:07,400
He's only famous for being wrong.

262
00:31:11,881 --> 00:31:30,960
But he was wrong in a kind of courageous and interesting way, because what he saw was that you can't just put the function predicate hammer or two hammer onto this blob and expect to capture common sense knowledge or human intelligence or make an artificial intelligence.

263
00:31:30,961 --> 00:31:33,840
It's much messier than that.

264
00:31:33,840 --> 00:31:48,481
And so Lynette said in 85 that he was going to start a research project which was to capture all of common sense knowledge in very complicated computer structures.

265
00:31:48,481 --> 00:31:51,441
And they weren't going to be just predicates.

266
00:31:51,441 --> 00:31:59,201
They were going to be whole bunches of interconnected predicates all referring to each other in all kinds of complicated ways.

267
00:31:59,200 --> 00:32:01,281
He called it psych.

268
00:32:01,921 --> 00:32:03,600
I don't remember why.

269
00:32:05,521 --> 00:32:16,241
I guess maybe because at one point when he started, he said that he was going to try to capture all of our knowledge, the knowledge that's in the Encyclopedia Britannica.

270
00:32:16,240 --> 00:32:19,441
At that point, I said, who cares about that?

271
00:32:19,441 --> 00:32:31,120
What's important is the knowledge that's not in the Encyclopedia Britannica, the common sense, everyday coping knowledge that's taken for granted by everybody who reads and writes the Encyclopedia Britannica.

272
00:32:31,120 --> 00:32:32,560
And then Lynette got it.

273
00:32:32,560 --> 00:32:44,800
And it became a kind of funny thing when he gave talks because he would sort of demonstrate the weird stuff that he has to put into his body of knowledge that isn't in the Encyclopedia Britannica.

274
00:32:44,801 --> 00:32:55,601
I remember one was that when, I don't know, whoever was president, somebody, when, say, Nixon was in Washington, his left foot was also in Washington.

275
00:32:55,600 --> 00:32:58,001
That's one of the facts you have to put in there.

276
00:32:58,001 --> 00:33:07,481
Or another funny thing, it was a kind of, it was kind of, the audience was in stitches because it's a kind of special humor to see the funny things that we take for granted.

277
00:33:07,481 --> 00:33:17,400
He had this story understanding program, and at one point it was trying to understand, he was trying to get it to understand that people could fall and drown.

278
00:33:17,401 --> 00:33:24,121
And so George fell into the river, gravity pulled him down, and he drowned.

279
00:33:24,120 --> 00:33:28,360
And the computer came out with the conclusion, and gravity drowned too.

280
00:33:30,600 --> 00:33:48,521
And that's, then you just can't, it gives you a sort of crazy feeling when you start realizing all the assumptions that we don't have to make because we have the familiarity with the world, that this program that he's making has to be making explicit.

281
00:33:48,521 --> 00:33:58,040
And so he believes that you've got to not just have definitions, but have what he calls frames.

282
00:33:58,040 --> 00:34:03,160
And a frame is, for instance, all that we know about hammers.

283
00:34:03,160 --> 00:34:09,801
That there are this many kinds of hammers and that they weigh this much and that this is how they break down and this is how much they cost.

284
00:34:09,961 --> 00:34:13,801
This is the kind of people who can use them and these are the kind of people who can't use them.

285
00:34:13,801 --> 00:34:16,521
Everything we know about hammers is going to go in there.

286
00:34:16,600 --> 00:34:22,681
So he thinks he's going to need millions of frames with thousands of facts in each.

287
00:34:22,680 --> 00:34:26,281
That's why I think he's courageous, though mad.

288
00:34:26,600 --> 00:34:37,961
At least he thinks he, Minsky said, oh, well, with in the in print, he said, well, when we get two million facts, then we'll be able to just have intelligent machines.

289
00:34:37,961 --> 00:34:41,800
And Lynnette knew that that wasn't the case, but Lynette was way off.

290
00:34:41,801 --> 00:34:44,560
He said, oh, well, we'll do that within 10 years.

291
00:34:43,560 --> 00:34:48,400
It's now been 20 years since 85 more, isn't it?

292
00:34:43,961 --> 00:34:50,401
85, 95.

293
00:34:50,640 --> 00:34:52,320
No, roughly 20 years.

294
00:34:52,321 --> 00:35:05,201
Lynette is still going on, still making, still putting in facts, having what he calls these cyclists, which are the sort of slaves who continually put in new information about hammers or whatever.

295
00:35:05,200 --> 00:35:08,641
And it's been 20 years and he isn't getting any closer.

296
00:35:08,640 --> 00:35:13,120
Husserl was making the same mistake, but he was a little more self-conscious about it.

297
00:35:13,361 --> 00:35:26,241
At some point, Husserl said that sort of spelling out the world in the see, Husserl thought we had a mental representation of the world.

298
00:35:26,240 --> 00:35:34,161
So we ought to be able to describe the system of beliefs which we have in our mind, whether there's any world or not.

299
00:35:34,160 --> 00:35:37,361
And then he discovered that was, quote, an infinite task.

300
00:35:37,361 --> 00:35:48,401
So he was discovering the same thing that Lynette was discovering, that it's this holism that Heidegger's talking about is something they can't get.

301
00:35:48,720 --> 00:36:16,080
But when I was involved in this, I sort of switched the question from how many facts you have to put in to a further problem, which I think Heidegger sees, that the more facts you put in, the harder it's going to be to get the relevant facts to come to bear on any particular sort of utterance about hammers or being a hammer.

302
00:36:16,080 --> 00:36:22,161
All these facts are supposed to become into the picture when they are relevant.

303
00:36:22,160 --> 00:36:29,040
But the trouble is there's it's hard to get the relevant ones, and now you've got millions of facts about hammers.

304
00:36:29,160 --> 00:36:38,681
Oh, when you're trying to understand somebody's particular relation to hammers and doing something or asking something or buying a hammer, which of those million facts are you going to get in?

305
00:36:38,680 --> 00:36:43,401
Well, Lynette said that must be a problem that's a problem of relevance, all right.

306
00:36:43,401 --> 00:36:45,960
So we better have some relevance maxims.

307
00:36:46,680 --> 00:36:51,161
And that's going to tell us how to determine which ones are relevant.

308
00:36:51,640 --> 00:36:55,320
And but oh, I skipped something.

309
00:36:55,961 --> 00:36:56,840
Do we need that?

310
00:36:56,841 --> 00:36:57,400
Let me think.

311
00:36:57,401 --> 00:36:58,360
Yeah, we need it.

312
00:36:58,361 --> 00:36:59,800
But I'll jump ahead.

313
00:37:00,600 --> 00:37:03,160
Let's do the relevance maxims now that I mentioned them.

314
00:37:03,160 --> 00:37:08,601
They don't work because what they are s we had many of them.

315
00:37:08,600 --> 00:37:24,841
And the typical ones are, you can imagine, well, if it's something near in time to this, then it's likely to, this fact is likely to be relevant to that fact, and not, and there's things are not likely to be relevant across a great deal of time.

316
00:37:25,321 --> 00:37:27,400
A lot easy to think of counteries.

317
00:37:27,881 --> 00:37:37,560
If you make a promise now to do something a year later, it's very relevant, even though it's not nearly as near as what's happening the same day.

318
00:37:37,560 --> 00:37:42,521
And of course, all kinds of things in history where people have done, made promise-like things.

319
00:37:42,521 --> 00:37:44,121
Anyway, you can imagine that.

320
00:37:44,120 --> 00:37:50,121
Or another, I was just thinking, he said, well, if it's in the same domain, it'll be relevant, and if not, not.

321
00:37:50,120 --> 00:37:51,560
And I made up the story.

322
00:37:51,560 --> 00:38:07,761
Well, suppose the domain is plants, and now, but you've got this case where there's a jockey about to ride a horse on a and but you but the betdor discovers that the jockey has hay fever and the field is full of goldenrod.

323
00:38:07,760 --> 00:38:22,801
So it's very relevant to know about the goldenrod, but it's hardly, but it doesn't seem to fit into the horses and betting domain at all, which is just meant to show that anything could be relevant to anything.

324
00:38:22,801 --> 00:38:26,881
And your relevant axioms or relevance axioms are never going to help.

325
00:38:26,881 --> 00:38:38,720
One way to see they're never going to help is, according to, I had lunch once with Gua, who was the, I forget his first name, who was Lynette's main assistant, and I asked him, well, what happened with these relevance axioms?

326
00:38:38,720 --> 00:38:40,321
He doesn't talk about them anymore.

327
00:38:40,321 --> 00:38:47,601
And Gua said, well, he got thousands and thousands of them, and then he didn't know which relevant maximums were relevant.

328
00:38:47,600 --> 00:38:50,640
And I thought that was a nice way.

329
00:38:50,640 --> 00:38:55,361
I mean, Heidegger would have liked that as a way that this could break down.

330
00:38:55,361 --> 00:39:04,560
And Fodor, whom we mentioned Jerry Fodor every once in a while now, Jerry Fodor, I mean, some of you will know him, but lots of you won't.

331
00:39:04,560 --> 00:39:15,521
He's a very, very good, famous philosopher, cognitive psychologist, who was also at MIT in the movie.

332
00:39:15,841 --> 00:39:35,040
Only he believed in some kind of cognitivist program, but he also was always sort of much more open to the problem that was lurking, the way Husserl was, than people like Lynette.

333
00:39:35,040 --> 00:39:50,401
Fodor says, in a book called The Modularity of Mind, but it doesn't matter where it comes out, he says, the problem, and this is the problem we've been talking about now: the whole holism of our system of beliefs about everything.

334
00:39:50,401 --> 00:39:57,521
The problem is to get the structure of an entire belief system to bear on individual occasions of belief fixation.

335
00:39:57,521 --> 00:40:02,201
That is, that's where believing that it's time to get a different hammer, for instance.

336
00:40:02,200 --> 00:40:02,840
Whatever.

337
00:40:02,841 --> 00:40:05,641
Some specific belief about something specific.

338
00:40:05,640 --> 00:40:08,360
Remember, I said, well, L, there are millions of facts.

339
00:40:08,361 --> 00:40:09,720
How do you get the relevant ones?

340
00:40:09,720 --> 00:40:18,521
Well, he says, we have, to put it bluntly, no computational formalism that shows us how to do this.

341
00:40:18,521 --> 00:40:27,161
That is, to see how the entire belief system bears on individual occasions.

342
00:40:27,481 --> 00:40:31,241
And we have no idea how such formalisms might be developed.

343
00:40:31,240 --> 00:40:33,000
This is the last sentence of his book.

344
00:40:33,001 --> 00:40:47,241
If someone, a Dreyfus, for example, were to ask us why we should even suppose that the digital computer is a plausible mechanism for the simulation of global cognitive processes, the answering silence would be deafening.

345
00:40:47,560 --> 00:41:00,121
And later, in reaction to Pinker, that some of you may know as the sort of guru of cognitivism now, Fodor wrote a book about four years ago called The Mind Doesn't Work Like That.

346
00:41:00,281 --> 00:41:08,601
The last sentence there is: What our cognitive science has done so far is mostly to throw some light on how much dark there is.

347
00:41:08,600 --> 00:41:14,761
So far, what our cognitive science has found out about the mind is mostly that we don't know how it works.

348
00:41:14,760 --> 00:41:16,920
Well, that's, Fodor is smart.

349
00:41:16,921 --> 00:41:19,560
Fodor understands that there's a big problem.

350
00:41:19,560 --> 00:41:22,281
He has no idea what to do about it.

351
00:41:22,281 --> 00:41:34,281
And that's sensible too if you're committed to having to find some model in which you have these formal relations, that is, logical relations between elementary facts.

352
00:41:35,481 --> 00:41:38,200
But he's always seeing that, gee, we don't know how to do this.

353
00:41:38,200 --> 00:41:40,841
We don't even know how to begin to do this.

354
00:41:40,841 --> 00:41:42,200
So now, where are we?

355
00:41:42,521 --> 00:41:47,841
I haven't let anybody say anything since I started talking 45 minutes ago.

356
00:41:44,040 --> 00:41:48,481
It's shocking.

357
00:41:48,640 --> 00:41:50,641
Doesn't anybody want to say anything?

358
00:41:51,120 --> 00:41:55,921
Sort of, I'm telling you a story that you have really no reason to have any opinion of.

359
00:41:57,040 --> 00:42:03,200
You have to take my word for it, unless you want to call in Minsky or somebody or Jerry Fodor.

360
00:42:03,200 --> 00:42:04,960
So I'm going on with my story.

361
00:42:06,240 --> 00:42:11,921
So now we've got something, one more thing I have to tell you: that there is something called the frame problem.

362
00:42:11,921 --> 00:42:20,401
That's not the problem of how to get everything into the hammer frame where every object has a frame.

363
00:42:20,401 --> 00:42:22,401
It's a special problem.

364
00:42:22,401 --> 00:42:35,680
And it's a problem that everybody sort of knew was going to be unanswerable by the people in artificial intelligence, the sort of Cartesian approach, but they just sort of repressed it.

365
00:42:35,680 --> 00:42:38,961
And then when I heard this frame problem, I thought, oh boy, there it is.

366
00:42:38,961 --> 00:42:40,641
That's what's going to get them.

367
00:42:40,640 --> 00:42:42,240
So what is the frame problem?

368
00:42:42,240 --> 00:42:55,121
Well, the computer has in it, like we have in our minds, according to this view, a representation of the current state of the world as a bunch of interrelated facts.

369
00:42:55,441 --> 00:42:58,881
And now suppose something changes in the world.

370
00:42:58,881 --> 00:43:03,601
Like, I don't know, I paint a wall blue in a room.

371
00:43:03,600 --> 00:43:12,161
How does the computer determine which of the represented facts stay the same and which representations have to be updated?

372
00:43:12,160 --> 00:43:14,721
It's another version of the relevance problem.

373
00:43:14,720 --> 00:43:19,760
When something changes, I mean, suppose, I mean, it's just incredible.

374
00:43:19,760 --> 00:43:23,521
I fold up my book and I put it over here.

375
00:43:23,841 --> 00:43:31,800
How many beliefs in my representation of this room can I leave the same, and how many do I have to change?

376
00:43:32,040 --> 00:43:35,321
There is no systematic way of dealing with it.

377
00:43:35,321 --> 00:43:42,921
And Minsky said, well, you've got to have a different kind of frame to deal with that.

378
00:43:42,921 --> 00:43:49,001
You've got to have frames that deal with typical situations, like going to a birthday party.

379
00:43:49,001 --> 00:44:02,601
And then you can organize all, you can, that will tell you what are the relevant facts at a birthday party, that you have to pay attention to the kids who are there, to the presents, to the ice cream and cake, to the parents.

380
00:44:02,600 --> 00:44:09,241
You don't have to pay attention to whether there are any goldfish in the house or whether it's sunny or not, and so forth.

381
00:44:09,240 --> 00:44:15,000
And interestingly enough, this is just something else that's sort of amazing in all this.

382
00:44:15,001 --> 00:44:17,720
Minsky came up with this idea of frames.

383
00:44:17,720 --> 00:44:21,241
I've never been able to prove what I'm about to say, except it's a fact.

384
00:44:21,240 --> 00:44:27,320
I had a student named Jim Geiser when I taught at MIT, who I taught Husserl to.

385
00:44:27,321 --> 00:44:46,441
Husserl, when he was dealing with this problem, came up with the idea of what he called frames, in which you could, in which in the frame for anything, there were the necessary conditions, the essential conditions of the thing, and then a lot of, what were there, were default conditions.

386
00:44:46,441 --> 00:44:52,201
And lo and behold, Minsky comes up with the same idea, only not for objects, but for situations.

387
00:44:52,200 --> 00:45:03,001
So the birthday party frame has in it all the essential features of birthdays, and then it has sort of the optional features of birthdays, like maybe whether there's a clown or not.

388
00:45:03,361 --> 00:45:05,720
And that, and Minsky called it all frames.

389
00:45:05,720 --> 00:45:18,080
So, lo and behold, Husserl is in there defending his side through Minsky against Heidegger, who's in there defending his side through me, and it seems to be a sort of head-on collision.

390
00:45:18,401 --> 00:45:35,361
But it turns out that you never could get the necessary somehow, no matter how many frames you made, you couldn't really deal with the frame problem.

391
00:45:35,361 --> 00:45:42,160
That's why, remember, he says, that's why the AI has really been brain dead since the 70s.

392
00:45:42,160 --> 00:45:45,841
The brain problem and common sense knowledge are the same problem.

393
00:45:46,240 --> 00:45:54,080
It's common sense knowledge, what's changed and what hasn't changed when I've moved my book from here to here and how to fix it.

394
00:45:54,080 --> 00:45:58,080
And just one second, let me see because I want to get this unit together.

395
00:45:59,441 --> 00:46:15,841
So, okay, so there's no way to deal with the frame problem that Minsky could figure out or anybody could figure out.

396
00:46:16,080 --> 00:46:18,560
Or Fodor thinks nobody will ever figure out.

397
00:46:18,560 --> 00:46:20,321
Yeah, no, go ahead.

398
00:46:27,040 --> 00:46:27,840
Yes.

399
00:46:28,481 --> 00:46:29,280
Yes.

400
00:46:33,120 --> 00:46:34,001
Good.

401
00:46:34,001 --> 00:46:35,040
That's right.

402
00:46:35,040 --> 00:46:36,080
So let me say that.

403
00:46:37,921 --> 00:46:46,241
If artificial intelligence means making computers that behave intelligently somehow, that's certainly not what I'm arguing against.

404
00:46:47,040 --> 00:47:07,080
The research program that I'm arguing against, which was the only one, the only game in town when I was at MIT, is now, you'd have to say, symbolic information processing, where, which is, or, as I say, my old student John Hoagland calls GoFi, good old-fashioned AI.

405
00:47:07,080 --> 00:47:29,161
That's a particular way of trying to make a computer intelligent, and it's a particular model of how we do it, namely by having representations of the world in our minds, in which all these facts are laid out, and rules for finding the relevant one, and rules for saying what changes when you change something.

406
00:47:29,160 --> 00:47:32,921
And that's the one that they had to give up on.

407
00:47:32,921 --> 00:47:42,521
Now, just to show you what else you could do, and it's in fact what they're doing, you could try to model the way the neurons do it.

408
00:47:42,521 --> 00:47:45,641
And then you'd have stimulated neural networks.

409
00:47:45,640 --> 00:47:57,640
And then you could try to teach simulated neural networks how to discriminate various classes of things by giving them descriptions of things and whether, and then they come up with it.

410
00:47:58,281 --> 00:47:59,721
Here's a successful one.

411
00:47:59,720 --> 00:48:11,720
There's a neural net that can distinguish the sound coming off rocks, the ping sound that comes off rocks with sonar, and the ping sound that comes off minds.

412
00:48:11,720 --> 00:48:16,601
So if you're going around looking for minds, you want to be able to do that.

413
00:48:16,600 --> 00:48:25,721
And there are human beings who can do that, but there are simulated neural nets that can be trained to do it and can do it as well or better than people.

414
00:48:25,720 --> 00:48:33,801
But they don't have any rules in them or any basic facts in them or any representations in them.

415
00:48:33,760 --> 00:48:38,280
They just, you just change all the weights on the neurons until it gets the right answer.

416
00:48:38,281 --> 00:48:41,961
And then you, I'm not going to tell you more about it, but it's in relation to him.

417
00:48:42,160 --> 00:48:46,720
So do you know the guy in Brown who does this by any chance?

418
00:48:46,961 --> 00:48:48,801
Anderson, I think it's?

419
00:48:49,120 --> 00:49:02,481
Yeah, because they're one of the famous neural net people who, during all the time that the Husserl Descartes people were thinking, was going on making simulated neural nets.

420
00:49:02,481 --> 00:49:04,961
And there's promise in that.

421
00:49:04,961 --> 00:49:09,760
And in fact, there's a funny way in which that turns out to be Heideggerian.

422
00:49:09,760 --> 00:49:11,040
We're about to get to that.

423
00:49:11,841 --> 00:49:14,001
There is a right way to think about this.

424
00:49:14,001 --> 00:49:30,321
And lo and behold, not only does Heidegger have the argument that good old-fashioned AI, symbolic information processing, based on mental representations or computer representations in them of the world, that's not going to work.

425
00:49:30,321 --> 00:49:33,120
But then there is something else that might work.

426
00:49:33,120 --> 00:49:34,320
So let's go back to that.

427
00:49:34,321 --> 00:49:36,080
Now let's see, where are we?

428
00:49:43,441 --> 00:49:47,600
Okay, yeah, Lynette's an oh, Gua said, did I tell you this?

429
00:49:47,600 --> 00:50:00,481
That Lynette lost touch with reality about eight years into this program, which, and it is, I think, amazing that he, in spite of the sort of evidently getting nowhereness of it, he goes on and on.

430
00:50:00,481 --> 00:50:16,001
But he has, he's written up something which is very relevant to us, which is an insight into why he thinks that it will work if you just keep trying hard enough, and why if he had taken this course, he would not have made this big mistake.

431
00:50:16,001 --> 00:50:18,400
So let's, if you read his Heidegger.

432
00:50:18,401 --> 00:50:20,161
So here's here's what happens.

433
00:50:20,160 --> 00:50:40,521
Lynette says, well, this is a big job, all right, to try to, in effect, construct all of the the world using only the quotes world, namely the facts about things and the and the beliefs about things and the and the brute facts about things.

434
00:50:40,521 --> 00:50:43,321
How why does he think that he can do it?

435
00:50:43,321 --> 00:50:46,121
Well, he thinks the following.

436
00:50:46,120 --> 00:50:48,841
And it's funny, he says this in AI magazine.

437
00:50:48,841 --> 00:50:50,361
Well, how should we do it?

438
00:50:50,361 --> 00:50:54,281
We should find out how people do it and program that.

439
00:50:54,600 --> 00:50:56,840
I mean, and that's now you have to translate that.

440
00:50:56,841 --> 00:51:09,001
That means, how do we, out of brute facts, make up a world in which the meaning relations and the relevant relations are captured in your computer model?

441
00:51:09,001 --> 00:51:25,400
And his answer is, well, since we experience brute facts and give all that meaning so that there is a world, all we have to do is figure out how people do that and program it.

442
00:51:25,720 --> 00:51:28,921
And that's his ultimate sort of optimism.

443
00:51:28,921 --> 00:51:31,240
Now, I'm going to stop and let somebody say something.

444
00:51:31,240 --> 00:51:34,600
Why is that sort of almost a joke?

445
00:51:34,600 --> 00:51:38,281
But it's a very sort of pedagogically interesting joke.

446
00:51:38,281 --> 00:51:40,441
So, problem, let me repeat.

447
00:51:40,441 --> 00:51:47,721
How do we manage to deal with all the huge number of brute facts in the world and give them all a meaning?

448
00:51:48,040 --> 00:51:55,161
Answer, well, I don't know, but humans obviously do it, so all we have to do is find out how humans do it.

449
00:51:55,160 --> 00:51:56,761
Where's the problem?

450
00:52:00,600 --> 00:52:03,560
Why is that interestingly wrong?

451
00:52:03,560 --> 00:52:04,680
You want to say something?

452
00:52:04,680 --> 00:52:05,960
You look like, yeah.

453
00:52:11,881 --> 00:52:13,241
Exactly.

454
00:52:13,240 --> 00:52:14,241
Exactly.

455
00:52:14,240 --> 00:52:15,040
We don't.

456
00:52:13,881 --> 00:52:16,961
I mean, that's the Cartesian assumption.

457
00:52:17,760 --> 00:52:25,121
And Thurley assumption that there are only brute facts in minds, and so minds have to assign function predicates to the brute facts.

458
00:52:25,120 --> 00:52:26,481
But that's wrong.

459
00:52:26,481 --> 00:52:28,641
That's bad phenomenology.

460
00:52:29,600 --> 00:52:32,401
There isn't any way humans do it.

461
00:52:33,040 --> 00:52:44,080
This question, how do we manage to represent and organize the vast array of facts that makes up common sense knowledge and find just those that are relevant in the current situation?

462
00:52:44,401 --> 00:52:47,281
That is, how do we give meaning to brute facts?

463
00:52:47,281 --> 00:52:49,681
And the answer is, we don't.

464
00:52:50,720 --> 00:52:53,121
That is, and what do we do then?

465
00:52:53,120 --> 00:52:55,760
Well, now we're connecting up with the course.

466
00:52:55,760 --> 00:52:57,920
What does Heidegger think we do?

467
00:53:00,481 --> 00:53:10,640
Well, the tricky thing is that if you look at the situation from outside in a detached theoretical way, there are only brute facts.

468
00:53:10,640 --> 00:53:13,841
And then you have to figure out how to give them meaning.

469
00:53:13,841 --> 00:53:18,641
But we are, to talk like Heidegger, always already in a situation.

470
00:53:18,640 --> 00:53:23,441
Familiarity is constitutive of our experience.

471
00:53:23,441 --> 00:53:31,840
There isn't, we never, except on weird special cases, have this problem of how to give some meaning.

472
00:53:31,841 --> 00:53:43,121
When you come across some strange thing that you don't know how it is, when you're doing sort of Egyptian archaeology, then you would have a brute fact and you would try to figure out what its meaning was.

473
00:53:43,120 --> 00:53:49,601
You would probably fail because you won't have the totality on which it fits, but at least you might have that question.

474
00:53:49,600 --> 00:53:53,361
But generally, of course, it's not mysterious.

475
00:53:53,361 --> 00:53:56,720
You grow up gradually crawling around.

476
00:53:56,720 --> 00:54:03,401
You discover what floors are for, and finally you discover what doors are for, and you discover how to open the doors.

477
00:54:04,441 --> 00:54:19,560
And once you have enough experience, and your neural net, whatever is going on in here, gets tuned somehow, which obviously is going to have an explanation, but it needn't have an explanation in terms of facts and representations.

478
00:54:19,881 --> 00:54:24,280
But once you get the right, once you have enough experience, then what?

479
00:54:24,281 --> 00:54:29,161
When it's hot in the room, windows just draw you to open them.

480
00:54:29,160 --> 00:54:33,801
When you want to get out of the room, doors just draw you to the to go out.

481
00:54:33,801 --> 00:54:39,001
And the whole world is always already organized in terms of familiarity.

482
00:54:39,001 --> 00:54:47,560
Now, one more thing, but of course, not the finished familiarity that you and I have, more or less already knowing lots and lots and lots, knowing it isn't even right.

483
00:54:48,040 --> 00:54:53,320
Seeing the world in such a way that what's relevant just pops out for us.

484
00:54:53,321 --> 00:54:54,600
If it's hot, it's windows.

485
00:54:54,600 --> 00:54:56,521
If it's leaving, it's doors.

486
00:54:59,080 --> 00:55:02,601
We don't have that sort of full blown.

487
00:55:02,600 --> 00:55:04,841
We gradually get it.

488
00:55:07,240 --> 00:55:11,240
But it looks like maybe I'm begging the question: how do we gradually get it?

489
00:55:11,240 --> 00:55:13,721
The answer is: I have no idea.

490
00:55:13,720 --> 00:55:17,001
I mean, the brain did it, does it somehow.

491
00:55:17,001 --> 00:55:22,200
Whatever it does, it does it somehow using the neurons and the connections on the neurons.

492
00:55:22,200 --> 00:55:30,040
So maybe if we could make a model of that, we could make something maybe on the scale of a lizard that did.

493
00:55:30,040 --> 00:55:41,241
There's a famous professor here, Walter Freeman, who thinks he can program something that will behave like a lizard, but he doesn't do it the way with internal representations.

494
00:55:42,120 --> 00:55:47,121
So here's, I'm just kind of, I'm not focusing enough for you.

495
00:55:47,120 --> 00:56:00,001
So we gradually, somehow, there's a process, and it doesn't involve, as far as anybody knows, rules or representations and beliefs and frames and such.

496
00:56:00,001 --> 00:56:04,881
Something goes on, and we get a familiarity with the world.

497
00:56:04,881 --> 00:56:10,161
And once you've got a familiarity with the world, then the things that are relevant just pop out at you.

498
00:56:10,160 --> 00:56:19,760
Now, Lynette begs the question, of course, when he says, well, humans give meaning to brute facts.

499
00:56:19,760 --> 00:56:24,880
Well, all we have to do is figure out how they give their meaning, so to speak, fact by fact to the brute facts.

500
00:56:25,521 --> 00:56:30,080
And we will then be able to do it.

501
00:56:30,080 --> 00:56:32,481
Well, that won't work, because that's not what we do.

502
00:56:32,481 --> 00:56:34,480
Now, I saw a hand of somebody.

503
00:56:34,481 --> 00:56:35,121
Yes.

504
00:56:36,240 --> 00:56:37,760
Well, we studied processing.

505
00:56:37,760 --> 00:56:52,640
A lot of times, very people started a lot more importantly, but just that time once it's worth it, it's all written down.

506
00:56:53,281 --> 00:56:55,521
Of course, something is going to fight.

507
00:56:58,640 --> 00:57:00,160
Yes, sort of.

508
00:57:00,481 --> 00:57:18,881
Let me just, I mean, there are models of how us and animals, and we're not any different than animals on this, gradually make sense of the world so that it shows up for us full of meaning.

509
00:57:18,881 --> 00:57:25,281
And we are, we need a new, we need a new bit of jargon and it's that's where Heidegger comes back in.

510
00:57:25,281 --> 00:57:34,360
The idea is not that we come to the world like minds from outside and gradually learn about things and learn about breakdown.

511
00:57:34,680 --> 00:57:41,080
This always already thing in, when you do it in a neural way, is called coupling.

512
00:57:41,080 --> 00:57:50,361
And the idea is that somehow the brain and the world are always already coupled so that there isn't any sort of mind coming in from outside.

513
00:57:50,361 --> 00:58:00,600
And in Walter Freeman, that means that this is just going to be jargon to you, but just so I can say something, he has models of rabbits smelling.

514
00:58:00,600 --> 00:58:03,641
That's what he's studied all his career.

515
00:58:03,640 --> 00:58:15,721
And when rabbits learn a new smell, and rabbits, when they get a smell, then the old Thackeray bulb in their brain goes into a certain attractor.

516
00:58:15,720 --> 00:58:18,921
That's just a certain energy state.

517
00:58:18,921 --> 00:58:27,881
And when they learn a new distinction, say between the smell of celery and the smell of turpentine, then they get another attract.

518
00:58:28,120 --> 00:58:31,001
And all these attractors get rearranged.

519
00:58:31,321 --> 00:58:36,040
There's this holism of attractors and distinctions and so forth.

520
00:58:36,040 --> 00:58:49,000
Anyway, and the whole idea, and the crucial thing that you have to get a feel for, is that the attractors are only there when the rabbit is there acting in the world.

521
00:58:49,801 --> 00:58:58,521
There's this coupling between the thing that has a smell and the state that the brain is going into.

522
00:58:59,080 --> 00:59:15,121
And it's that always already, you can't understand what's going on in the brain except that it's interacting with the carrots or whatever, and you can't, the animal is, and you can't understand interacting with the carrots except in terms of all the other things the animal has learned to interact with.

523
00:59:14,441 --> 00:59:20,080
That's thought considered Heideggerian AI.

524
00:59:20,801 --> 00:59:36,241
In fact, in Freeman's case, he considers it Merlopontean AI because Merilloponty is much interested in what we share with animals in having something like an environment anyway where we can s distinguish things.

525
00:59:36,240 --> 00:59:44,401
And Heideggerian level, nobody's trying to program how beings take a stand on their own being.

526
00:59:45,040 --> 00:59:50,400
But if we could just get a clue as to the familiarity, I mean animals have familiarity too.

527
00:59:50,401 --> 00:59:57,361
You don't need to have a familiarity in which things connect up with your self-interpretation.

528
00:59:57,361 --> 01:00:01,681
It's enough if the remember there are two for the sake ofs.

529
01:00:01,680 --> 01:01:09,560
Animals, beavers share the for the sake of of building huts and they need shelter and that's the kind of thing that Merlopony is interested in and that's the kind of thing somebody like Walter would try to understand how the brain does it and nobody and that makes it us interesting and mysterious can even imagine how the brain is going to make this all connect up with somebody taking a stand on their identity but in a certain sense of course unless you are signed mystical you think the brain does it what else could do it is my feeling I mean you might think that the soul does it and if you do that's fine, but the point is simply, just from what I'm saying now, and I'm going to stop saying it: is that there are people with other models and they are they think they're Heideggerian because they think that the organism is coupled with the world from the start, it's always already in it, and then as Wittgenstein puts it, light dawns gradually over the whole.

530
01:01:09,560 --> 01:01:14,761
You get a more and more refined holism, whether you're a rabbit or us.

531
01:01:14,760 --> 01:01:15,721
Yeah.

532
01:01:24,760 --> 01:01:26,280
Yes, how is that going to be relevant?

533
01:01:26,281 --> 01:01:36,441
She wants to talk about Oliver Sachs, who wrote a book, The Man Who Mistook His Wife for a Hat, and writes for The New Yorker about all kinds of weird brain pathology.

534
01:01:36,600 --> 01:01:38,281
I liked him a lot.

535
01:01:45,321 --> 01:01:47,560
Yes, the story about the glove.

536
01:01:47,560 --> 01:01:49,161
Right, good, good.

537
01:01:49,160 --> 01:01:50,200
Yes, it's amazing.

538
01:01:50,200 --> 01:01:54,361
It sounds so Husserian and so wrong-headed.

539
01:01:54,361 --> 01:02:02,121
I mean, this guy is got such a brain damage that he's almost become a phenomenologist as opposed to a Heideggerian phenomenologist.

540
01:02:02,120 --> 01:02:04,841
He's almost become a Husserlian phenomenologist.

541
01:02:04,841 --> 01:02:13,080
When asked, when shown a glove, he doesn't see it as something to put on your hand in colder weather or anything.

542
01:02:13,080 --> 01:02:18,841
He says, I see it's an object with five outpouchings.

543
01:02:20,120 --> 01:02:21,641
Ah, it's a glove.

544
01:02:21,640 --> 01:02:29,801
He deduces from its sort of properties its function in a non-worldlike way.

545
01:02:29,801 --> 01:02:38,280
And this poor guy is so far gone that somehow I forget how he manages to misdeduce his wife and think that she's a hammer.

546
01:02:38,481 --> 01:02:43,521
And he tries to put his wife on as they're leaving the office.

547
01:02:43,160 --> 01:02:45,521
But that's pretty extreme.

548
01:02:45,841 --> 01:02:54,321
But in any case, yes, there are pathologies in which people lose being in the world.

549
01:02:54,321 --> 01:02:58,080
That's a very funny and funny isn't the right word, weird thing.

550
01:02:58,321 --> 01:03:00,881
And Sachs is interested in it.

551
01:03:01,040 --> 01:03:02,881
Did I see another hand?

552
01:03:03,200 --> 01:03:04,560
Okay, well, yeah.

553
01:03:09,600 --> 01:03:10,001
Yeah.

554
01:03:10,321 --> 01:03:11,201
Yeah.

555
01:03:21,040 --> 01:03:22,080
Say the things louder.

556
01:03:22,080 --> 01:03:23,681
I have trouble hearing you.

557
01:03:23,680 --> 01:03:25,281
And I'll repeat it.

558
01:03:28,481 --> 01:03:29,040
That's right.

559
01:03:29,361 --> 01:03:38,240
Heidegger, let me just say, Heidegger rejects the idea that, of course, in weird special occasions, again, you get a brute fact and have to deal with it, like archaeologists.

560
01:03:38,240 --> 01:03:40,240
But generally, that's all.

561
01:03:40,240 --> 01:03:45,760
We're always already in a world in which the background is not brute facts.

562
01:03:45,760 --> 01:03:47,040
Yeah, go ahead.

563
01:03:58,080 --> 01:04:01,361
The brute facts aren't part of the referential totality.

564
01:04:01,361 --> 01:04:11,760
They're what's left when the referential totality either breaks down, as when something's missing in the workshop, and then there's all this stuff around that you can't make any use of.

565
01:04:11,760 --> 01:04:16,801
Of course, you still know what it's for, but it's sort of half broken down.

566
01:04:17,441 --> 01:04:21,361
But normally, we never encounter group facts.

567
01:04:21,361 --> 01:04:24,800
And even, of course, the archaeologist isn't really encountering group facts.

568
01:04:24,801 --> 01:04:29,521
He knows that this is some kind of equipment which relates to other equipment and so forth.

569
01:04:29,881 --> 01:04:38,441
So we are always, the crucial thing is we're always already in a world in which lots of stuff is interconnected with lots of other stuff.

570
01:04:38,441 --> 01:04:40,521
And we don't even have to be conscious of it.

571
01:04:40,521 --> 01:04:41,800
That's what you were saying too.

572
01:04:41,801 --> 01:04:48,681
We just automatically cope with it so that the relevant things just show up for us.

573
01:04:48,680 --> 01:04:52,121
So am I answering you or just missing what you're saying?

574
01:05:02,361 --> 01:05:02,920
That's right.

575
01:05:02,921 --> 01:05:08,441
It is an order of priority that people like Whisero and Searle think you start with root facts.

576
01:05:08,441 --> 01:05:17,080
That's what remember the joke, the ironic joke in Heidegger says, you start with group facts, then you add meaning and function predicates, and that's easy.

577
01:05:17,080 --> 01:05:18,040
Why not?

578
01:05:18,040 --> 01:05:20,281
That's sort of the picture.

579
01:05:20,281 --> 01:05:23,721
But that's not how, but the phenomenology suggests.

580
01:05:23,720 --> 01:05:33,880
And that's, again, I keep wanting to repeat, the crucial thing is that you're already, always, already in a meaningful whole, from the moment you're born.

581
01:05:33,881 --> 01:05:36,121
It just isn't very structured yet.

582
01:05:36,120 --> 01:05:39,560
And you're never standing outside it.

583
01:05:39,560 --> 01:05:40,600
Well, that's not right.

584
01:05:41,321 --> 01:05:51,080
And we are, maybe this is something you want to say, we're capable of taking a theoretical attitude in which you do just stand outside and look at root facts.

585
01:05:51,080 --> 01:05:52,600
That's the research program.

586
01:05:52,600 --> 01:05:56,600
That leads to the research program of good old-fashioned AI.

587
01:05:56,921 --> 01:06:03,080
But that is, one, it's a derivative something we do.

588
01:06:03,081 --> 01:06:08,280
If it really is extreme, it's pathological, like the man who mistakes his wife for a hat.

589
01:06:08,281 --> 01:06:21,280
And in any case, if you do it, you can't use the pieces that you've got when you take this theoretical attitude and step out of the situation to recreate the situation.

590
01:06:21,281 --> 01:06:26,240
That's the claim that you can't build up the holism out of these elements.

591
01:06:26,241 --> 01:06:39,840
Even though you can get, even if you could get to the elements, the minimal, meaningless bits, they wouldn't do you any good because you couldn't put them together and, for instance, relevance would be missing.

592
01:06:39,840 --> 01:06:40,240
Yeah.

593
01:06:45,201 --> 01:06:58,880
You have the part where you assign need things to get us for a bunch of gatherers in the jungle for those Asian views and try to explain the development of how you bring once the assigned needs be new back.

594
01:06:58,880 --> 01:07:02,640
Does Heidegger have a story of getting this from theory?

595
01:07:04,241 --> 01:07:19,600
Ah, no, Heidegger doesn't have a story about how, for instance, the institutions that make, say, money possible, which is an example that he uses a lot, but he does think, and how many have seen Searle's, you know, money example.

596
01:07:19,600 --> 01:07:26,880
Here's a piece of paper who died in various ways, piece of cellulose, and what makes it money?

597
01:07:26,880 --> 01:07:34,401
Well, it takes the fact that there are institutions and practices that treat this piece of cellulose as money.

598
01:07:34,401 --> 01:07:36,720
That is, they give it that function.

599
01:07:36,720 --> 01:07:40,240
And that's okay.

600
01:07:41,441 --> 01:07:46,880
But if you try to make a whole, and as a philosophical position, you could have it.

601
01:07:46,880 --> 01:07:55,521
But it's, and in Searle, it's all on a background, and that makes it much more close to Heidegger than, say, in Husserl or Lynat.

602
01:07:55,760 --> 01:08:00,920
But the problem is, or the question is, how do we see this?

603
01:08:00,920 --> 01:08:05,960
Do we see this as a piece of paper to which we then attribute it being money?

604
01:08:05,960 --> 01:08:19,401
And I want to say, Searle and I've had this debate: no, we see it as looking valuable, unlike the way the kind of, say, big bills before there were Euros in bright colors look valuable.

605
01:08:19,481 --> 01:08:22,280
They don't, I mean, we see it already.

606
01:08:23,481 --> 01:08:32,281
Kids are always already in the world, and by the time they see any money at all, it doesn't look like cellulose with colored spots on it.

607
01:08:32,281 --> 01:08:33,560
It looks valuable.

608
01:08:33,560 --> 01:08:41,160
And I don't know if Searle needs to disagree, because he's going to give a logical analysis of what's going on, not a phenomenology, he says.

609
01:08:49,000 --> 01:08:52,840
You're margins, where you're just describing these theories.

610
01:08:53,000 --> 01:08:56,521
He's trying to explain.

611
01:08:56,521 --> 01:08:56,920
Right.

612
01:08:56,920 --> 01:09:08,361
I mean, if you want to explain how these institutions, these complicated institutions develop and what intentionality is, how intentionality is involved in them, that's right, that's Searle's project.

613
01:09:08,361 --> 01:09:17,160
But if you're going to do the ontology, then Searle's a Cartesian, because if you ask, well, what is there finally?

614
01:09:17,161 --> 01:09:21,160
The answer is brute facts and minds with intentionality.

615
01:09:21,161 --> 01:09:28,361
And he's going to analyze everything in terms of mind to world direction to fit, world to mind, direction to fit.

616
01:09:28,361 --> 01:09:30,441
And that's a different project.

617
01:09:30,840 --> 01:09:32,840
Okay, I want to, you want to say something?

618
01:09:32,840 --> 01:09:33,321
Good.

619
01:09:33,560 --> 01:09:33,880
Yeah.

620
01:09:41,241 --> 01:09:44,160
Whether we could ever experience it.

621
01:09:43,401 --> 01:09:46,720
Well.

622
01:09:49,920 --> 01:09:53,040
Well, I think that's right and not quite right.

623
01:09:53,041 --> 01:10:08,401
I mean, it may be that once we're always already in the world, we can never totally deworld so that we can get, no, no, we can take a theoretical attitude, I think, and get brute facts.

624
01:10:08,401 --> 01:10:18,000
You can get cloud chamber paths or something, and they only mean something in a theory.

625
01:10:18,720 --> 01:10:19,840
This is for later.

626
01:10:19,840 --> 01:10:25,040
Because, I mean, this is about to do what science, whether science gets brute facts, and when they do, what do they do with them?

627
01:10:25,041 --> 01:10:28,800
Well, they use them to find out about brute objects like electrons.

628
01:10:28,800 --> 01:10:34,081
And I think Heidegger thinks that's all terrific, but that's derivative from the...

629
01:10:34,081 --> 01:10:36,481
Remember, we're back at the very beginning of the lecture.

630
01:10:36,481 --> 01:10:47,521
It's only because we have a world and we can deal with it ready to hand that we can finally, by a lot of deworlding, get to the brute facts.

631
01:10:48,000 --> 01:10:50,401
But they are, or maybe never.

632
01:10:50,401 --> 01:10:51,201
But, yeah.

633
01:10:52,960 --> 01:11:00,321
It seems for Heidegger that the meaning, the new meaning can only be generated out of an already systematic.

634
01:11:01,281 --> 01:11:02,960
I think that's right.

635
01:11:05,840 --> 01:11:15,280
Well, you talk about it as sort of a quasi-meaning that gets clarified.

636
01:11:15,281 --> 01:11:23,120
It all depends, and we have to put it off until we get to the reality chapter on what does it mean to de-world, which theory can do.

637
01:11:23,441 --> 01:11:34,920
And if theory can de-world, it looks like you can cause a kind of systematic breakdown that will reveal the brute facts to you, I think.

638
01:11:34,920 --> 01:11:36,281
But we have to look and see.

639
01:11:37,401 --> 01:11:39,000
We may disagree about that.

640
01:11:39,000 --> 01:11:43,960
I want to make sure that I get the next five minutes to finish this because we're not going to go back to it.

641
01:11:43,960 --> 01:11:45,160
So let's see.

642
01:11:47,000 --> 01:11:50,120
So we've got to, I want to see if we've done all these quotes.

643
01:11:50,121 --> 01:11:51,560
Yeah, there's the Husserl quote.

644
01:11:51,560 --> 01:11:52,600
There's the Fodor quote.

645
01:11:52,600 --> 01:11:54,600
No, no, there's still one, the punchline.

646
01:11:54,600 --> 01:11:57,320
We've got to get to Heideggerian AI.

647
01:11:57,321 --> 01:12:01,800
And that is, I'm rushing to Heideggerian AI.

648
01:12:02,441 --> 01:12:11,481
There is a book out this year called, it looks like this: Reconstructing the Cognitive World by Michael Wheeler.

649
01:12:11,481 --> 01:12:18,760
And I will put my copy on reserve in the library, unless I can get the library to get it in a hurry.

650
01:12:18,760 --> 01:12:26,361
What's interesting about this book is it says where AI is now is Heideggerian AI.

651
01:12:26,361 --> 01:12:28,920
And that's his thesis in this book.

652
01:12:28,920 --> 01:12:31,160
And where's my little quote?

653
01:12:31,161 --> 01:12:34,600
Because, yeah, here we are at the bottom.

654
01:12:35,640 --> 01:12:36,280
No?

655
01:12:37,241 --> 01:12:38,120
No?

656
01:12:38,680 --> 01:12:40,120
I guess I didn't.

657
01:12:40,121 --> 01:12:41,321
Oh, this is annoying.

658
01:12:41,321 --> 01:12:44,280
I didn't put that quote in, but I'll put it in right now.

659
01:12:44,281 --> 01:12:45,240
Where are we?

660
01:12:45,960 --> 01:12:47,080
I think.

661
01:12:47,640 --> 01:13:00,840
A Heideggerian quoting him: a Heideggerian cognitive science is emerging right now in the laboratories and offices around the world where embodied embedded thinking is under active investigation and development.

662
01:13:00,840 --> 01:13:08,920
That embodied embedded is the always-already in the world coupling kind of AI that he's talking about there.

663
01:13:09,560 --> 01:13:14,080
And now these two things come together in this interesting way.

664
01:13:13,640 --> 01:13:20,960
And after the failure of good old-fashioned AI, Minsky was no longer head of MIT.

665
01:13:21,281 --> 01:13:26,160
I was at Berkeley, and they called me back to give a talk to the AI people.

666
01:13:26,481 --> 01:13:31,600
And I gave them a talk, why AI researchers should study being in time.

667
01:13:31,600 --> 01:13:35,441
And that's where finally these two came together.

668
01:13:35,441 --> 01:13:44,481
And I said, quote, the meaningful objects among which we live are not a model of the world stored in the mind, but they're the world itself.

669
01:13:44,481 --> 01:13:57,200
And that very same year, Rodney Brooks became head, moved from Stanford to MIT, became head of the MIT AI Lab, and declared, quote, that the best model of the world is the world itself.

670
01:13:57,521 --> 01:14:09,840
And so the and he says, in a quote that I haven't got here, people tell him that that's Heideggerian, but he figured it out for himself.

671
01:14:09,840 --> 01:14:13,280
And I'm not going to try to settle that.

672
01:14:13,281 --> 01:14:18,320
But what he says about the frame problem is interesting, and I want to read it.

673
01:14:18,321 --> 01:14:23,600
He says, he makes robots, and they don't face the frame problem.

674
01:14:23,600 --> 01:14:24,560
Why not?

675
01:14:24,560 --> 01:14:27,200
Because they don't have representations of the world.

676
01:14:27,201 --> 01:14:32,000
They just have cameras that keep track of the world and microphones that keep track of the world.

677
01:14:32,000 --> 01:14:36,080
And he says, why could my simulated robot handle the frame problem?

678
01:14:36,401 --> 01:14:39,040
Because it was using the world as its own model.

679
01:14:39,041 --> 01:14:46,080
It never referred to an internal description of the world that would get out of date if anything in the real world moved.

680
01:14:46,081 --> 01:14:52,880
So you answer the frame problem by making something that keeps track of what's going on in the world.

681
01:14:52,880 --> 01:14:54,321
Now, has he done it?

682
01:14:54,321 --> 01:14:56,320
Has he made Heideggerian AI?

683
01:14:56,321 --> 01:15:00,200
No, because there's one basic thing missing.

684
01:14:59,361 --> 01:15:06,441
His robots only respond to fixed features of the world, like the pop bottles.

685
01:15:06,521 --> 01:15:13,800
One of them, one robot goes around the AI lab picking up pop bottle cans or pop cans or something, and it's certain features.

686
01:15:13,800 --> 01:15:19,880
He calls these robots animats because he understands that they're like insects.

687
01:15:19,880 --> 01:15:25,560
They don't learn, they don't have familiarity, they just respond.

688
01:15:25,560 --> 01:15:32,920
So he hasn't really solved the, he solved the frame problem, but he's sort of changed the subject.

689
01:15:32,920 --> 01:15:43,320
What he hasn't got is the following fact or following phenomenon: that as we learn more and more about the world, it gets more and more familiar.

690
01:15:43,321 --> 01:15:49,080
And things look like, windows look like what they draw us when it's hot.

691
01:15:49,081 --> 01:15:51,560
A Merleau-Ponty example is useful.

692
01:15:51,560 --> 01:15:56,840
When we learn our way around in a city, it starts looking relatively meaningless.

693
01:15:56,840 --> 01:16:02,201
And as we get used to it, we actually see the relevance, familiarity of it.

694
01:16:02,361 --> 01:16:08,121
This corner is where you turn to go to the bakery, and that corner is where you catch the bus, and you don't even have to think it.

695
01:16:08,121 --> 01:16:11,720
It just looks bus-catching and laundry-like.

696
01:16:11,720 --> 01:16:15,720
And that's what the animats don't have.

697
01:16:16,041 --> 01:16:19,640
So, in a way, they've made a big step.

698
01:16:19,640 --> 01:16:37,720
They've abandoned symbolic representations, they've abandoned cognitivism, but they've ended up in a kind of brute of empiricist behaviorism now, in which these animats are just responding to fixed features of the environment and aren't learning anything.

699
01:16:37,720 --> 01:16:40,361
So, we have to stop with that.

700
01:16:40,680 --> 01:17:06,800
And the answer is that once you see the Heideggerian phenomenon, you see both the good old-fashioned AI couldn't work, and you see that the new kind of Rodney Brooks MIT AI for robots isn't going to capture our kind of intelligence and our kind of phenomenon either.

701
01:17:06,800 --> 01:17:23,040
And but I guess I should say, Michael Wheeler has a whole lot to say about all the various versions of the coupling of brain and world, which he thinks of as Heideggerian AI, and thinks that they're finally on the right track.

702
01:17:23,201 --> 01:17:27,521
Somebody wants to say one thing.

703
01:17:27,840 --> 01:17:29,040
Well, no, I said that.

704
01:17:29,041 --> 01:17:36,800
I don't know how to, I don't see any way to make a robot concerned about its own being, but then we're a material thing, and we're concerned about our own being.

705
01:17:36,800 --> 01:17:42,320
So it looks like at least you could make them something that's concerned about its own being.

706
01:17:46,481 --> 01:17:47,521
Wait, I can't hear you.

707
01:17:47,521 --> 01:17:48,560
Please stop with that.

708
01:17:48,880 --> 01:17:49,760
We'll talk again.

709
01:17:49,760 --> 01:17:50,960
I'll talk afterward.

710
01:17:50,960 --> 01:17:53,840
Okay, read space for next week.

711
01:17:57,361 --> 01:18:00,560
Now, come up because I can't hear you.

712
01:18:15,081 --> 01:18:18,200
I think that's what I'm doing.
